"use strict";(globalThis.webpackChunkedge_ai=globalThis.webpackChunkedge_ai||[]).push([[154],{5415:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>a,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"face_authentication/image_processing","title":"Exercise 01. Image Processing and Normalization","description":"Overview","source":"@site/docs/face_authentication/1_image_processing.md","sourceDirName":"face_authentication","slug":"/face_authentication/image_processing","permalink":"/docs/face_authentication/image_processing","draft":false,"unlisted":false,"editUrl":"https://github.com/Wyliodrn/edge-ai-101/tree/main/docs/face_authentication/1_image_processing.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Overview","permalink":"/docs/face_authentication/"},"next":{"title":"Exercise 02. ConvNeXt Model and Embedding Generation","permalink":"/docs/face_authentication/embeddings"}}');var r=i(4848),t=i(8453);const a={},o="Exercise 01. Image Processing and Normalization",l={},d=[{value:"Overview",id:"overview",level:2},{value:"Understanding Tensors and Image Processing",id:"understanding-tensors-and-image-processing",level:2},{value:"What is a Tensor?",id:"what-is-a-tensor",level:3},{value:"What is Normalization?",id:"what-is-normalization",level:3},{value:"Why Use Mean and Standard Deviation?",id:"why-use-mean-and-standard-deviation",level:3},{value:"Why ImageNet Normalization is Critical for ConvNeXt",id:"why-imagenet-normalization-is-critical-for-convnext",level:2},{value:"Your Task",id:"your-task",level:2},{value:"Implementation Steps",id:"implementation-steps",level:2},{value:"Implementation Approach:",id:"implementation-approach",level:3},{value:"Key Operations Needed:",id:"key-operations-needed",level:3},{value:"Testing",id:"testing",level:2},{value:"Expected Output Format",id:"expected-output-format",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"exercise-01-image-processing-and-normalization",children:"Exercise 01. Image Processing and Normalization"})}),"\n",(0,r.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,r.jsxs)(n.p,{children:["This exercise teaches you how to properly preprocess images for computer vision models, specifically focusing on ImageNet normalization. You'll implement the ",(0,r.jsx)(n.code,{children:"image_with_std_mean"})," function that transforms raw images into model-ready tensors."]}),"\n",(0,r.jsx)(n.h2,{id:"understanding-tensors-and-image-processing",children:"Understanding Tensors and Image Processing"}),"\n",(0,r.jsx)(n.h3,{id:"what-is-a-tensor",children:"What is a Tensor?"}),"\n",(0,r.jsxs)(n.p,{children:["A ",(0,r.jsx)(n.strong,{children:"tensor"})," is a multi-dimensional array that serves as the fundamental data structure in machine learning. Think of it as:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"1D tensor"}),": A vector (like ",(0,r.jsx)(n.code,{children:"[1, 2, 3, 4]"}),")"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"2D tensor"}),": A matrix (like a spreadsheet with rows and columns)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"3D tensor"}),": A cube of data (like our image with height \xd7 width \xd7 channels)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"4D tensor"}),": A batch of 3D tensors (multiple images)"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["For images, we use ",(0,r.jsx)(n.strong,{children:"3D tensors"})," with dimensions:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Channels"}),": Color information (3 for RGB: Red, Green, Blue)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Height"}),": Number of pixel rows"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Width"}),": Number of pixel columns"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["ConvNeXt expects tensors in ",(0,r.jsx)(n.strong,{children:'"channels-first"'})," format: ",(0,r.jsx)(n.code,{children:"(channels, height, width)"})," rather than ",(0,r.jsx)(n.code,{children:"(height, width, channels)"}),"."]}),"\n",(0,r.jsx)(n.h3,{id:"what-is-normalization",children:"What is Normalization?"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Normalization"})," transforms data to have consistent statistical properties. For images, we perform two types:"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Scale Normalization"}),": Convert pixel values from ",(0,r.jsx)(n.code,{children:"[0-255]"})," to ",(0,r.jsx)(n.code,{children:"[0-1]"})," by dividing by 255"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Statistical Normalization"}),": Transform to have zero mean and unit variance using: ",(0,r.jsx)(n.code,{children:"(value - mean) / standard_deviation"})]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"why-use-mean-and-standard-deviation",children:"Why Use Mean and Standard Deviation?"}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.strong,{children:"ImageNet mean and standard deviation"})," values aren't arbitrary - they're computed from millions of natural images:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsxs)(n.strong,{children:["Mean ",(0,r.jsx)(n.code,{children:"[0.485, 0.456, 0.406]"})]}),": Average pixel values across Red, Green, Blue channels"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsxs)(n.strong,{children:["Std ",(0,r.jsx)(n.code,{children:"[0.229, 0.224, 0.225]"})]}),": Standard deviation for each channel"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Why these specific values matter for ConvNeXt:"})}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Distribution Matching"}),": ConvNeXt was trained on ImageNet data with these exact statistics. Using different values would be like speaking a different language to the model."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Zero-Centered Data"}),": Subtracting the mean centers pixel values around zero, which helps neural networks learn faster and more stably."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Unit Variance"}),": Dividing by standard deviation ensures all channels contribute equally to learning, preventing one color channel from dominating."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Gradient Flow"}),": Normalized inputs lead to better gradient flow during training, preventing vanishing or exploding gradients."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"why-imagenet-normalization-is-critical-for-convnext",children:"Why ImageNet Normalization is Critical for ConvNeXt"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"ImageNet normalization is essential for four key reasons:"})}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Neural Network Stability"}),": Raw pixel values (0-255) are too large and cause training instability. Normalizing to smaller ranges helps gradients flow properly during backpropagation."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Pre-trained Model Compatibility"}),": ConvNeXt models are trained on ImageNet-normalized data. Using the same normalization ensures your input matches what the model expects - like using the same units of measurement."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Feature Standardization"}),": Different color channels have different statistical distributions in natural images. Per-channel normalization gives equal importance to all color information."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Mathematical Optimization"}),": The normalization formula ",(0,r.jsx)(n.code,{children:"(pixel/255 - mean) / std"})," transforms arbitrary pixel values into a standardized range that neural networks can process efficiently."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Without proper normalization, ConvNeXt will produce poor results"})," because the input distribution doesn't match its training data - imagine trying to use a thermometer calibrated in Celsius to read Fahrenheit temperatures!"]}),"\n",(0,r.jsx)(n.h2,{id:"your-task",children:"Your Task"}),"\n",(0,r.jsxs)(n.p,{children:["Implement the ",(0,r.jsx)(n.code,{children:"image_with_std_mean"})," function that:"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Resizes"})," the input image to the specified resolution using Triangle filtering"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Converts"})," to RGB8 format to ensure consistent color channels"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Creates"})," a tensor with shape ",(0,r.jsx)(n.code,{children:"(3, height, width)"})," - channels first format"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Normalizes"})," pixel values from [0-255] to [0-1] range"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Applies"})," ImageNet standardization: ",(0,r.jsx)(n.code,{children:"(pixel/255 - mean) / std"})]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"implementation-steps",children:"Implementation Steps"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-rust",children:"pub fn image_with_std_mean(\n    img: &DynamicImage,\n    res: usize,\n    mean: &[f32; 3],\n    std: &[f32; 3],\n) -> Result<Tensor>\n"})}),"\n",(0,r.jsx)(n.h3,{id:"implementation-approach",children:"Implementation Approach:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Resize Image"}),": Use appropriate image resizing methods"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Convert Format"}),": Ensure consistent color channel format"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Extract Data"}),": Get raw pixel data from the image"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Create Tensor"}),": Build tensor with correct shape and dimensions"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Normalize"}),": Apply scaling and ImageNet standardization"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"key-operations-needed",children:"Key Operations Needed:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Image resizing and format conversion"}),"\n",(0,r.jsx)(n.li,{children:"Tensor creation from raw data"}),"\n",(0,r.jsx)(n.li,{children:"Dimension reordering (channels-first format)"}),"\n",(0,r.jsx)(n.li,{children:"Mathematical operations for normalization"}),"\n",(0,r.jsx)(n.li,{children:"Broadcasting for per-channel operations"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Hint"}),": Check the CHEATSHEET.md for specific API calls and tensor operations."]}),"\n",(0,r.jsx)(n.h2,{id:"testing",children:"Testing"}),"\n",(0,r.jsx)(n.p,{children:"The test verifies that:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Tensor values are in the expected normalized range (approximately [-2.5, 2.5])"}),"\n",(0,r.jsx)(n.li,{children:"Values are actually normalized (not just zeros or ones)"}),"\n",(0,r.jsx)(n.li,{children:"The transformation follows ImageNet standards"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Run the test with:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"cargo test\n"})}),"\n",(0,r.jsx)(n.h2,{id:"expected-output-format",children:"Expected Output Format"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Input"}),": DynamicImage of any size"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Output"}),": Tensor with shape ",(0,r.jsx)(n.code,{children:"(3, 224, 224)"})," and ImageNet-normalized values"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Value Range"}),": Approximately [-2.12, 2.64] based on ImageNet constants"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"This preprocessing step is crucial for the face authentication pipeline, as it ensures images are in the exact format expected by the ConvNeXt model in the next exercise."})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>o});var s=i(6540);const r={},t=s.createContext(r);function a(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);