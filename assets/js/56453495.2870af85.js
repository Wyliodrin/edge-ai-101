"use strict";(globalThis.webpackChunkedge_ai=globalThis.webpackChunkedge_ai||[]).push([[796],{7458:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>o,contentTitle:()=>d,default:()=>h,frontMatter:()=>l,metadata:()=>s,toc:()=>a});const s=JSON.parse('{"id":"face_authentication/embeddings","title":"Exercise 02. ConvNeXt Model and Embedding Generation","description":"Overview","source":"@site/docs/face_authentication/2_embeddings.md","sourceDirName":"face_authentication","slug":"/face_authentication/embeddings","permalink":"/docs/face_authentication/embeddings","draft":false,"unlisted":false,"editUrl":"https://github.com/Wyliodrin/edge-ai-101/edit/main/docs/face_authentication/2_embeddings.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Exercise 01. Image Processing and Normalization","permalink":"/docs/face_authentication/image_processing"},"next":{"title":"Exercise 03: Cosine Similarity for Face Authentication","permalink":"/docs/face_authentication/similarity"}}');var t=i(4848),r=i(8453);const l={},d="Exercise 02. ConvNeXt Model and Embedding Generation",o={},a=[{value:"Overview",id:"overview",level:2},{value:"What is ConvNeXt?",id:"what-is-convnext",level:2},{value:"Key Features of ConvNeXt:",id:"key-features-of-convnext",level:3},{value:"ConvNeXt-Atto Variant:",id:"convnext-atto-variant",level:3},{value:"What are Face Embeddings?",id:"what-are-face-embeddings",level:2},{value:"Purpose of Face Embeddings:",id:"purpose-of-face-embeddings",level:3},{value:"Properties of Good Face Embeddings:",id:"properties-of-good-face-embeddings",level:3},{value:"Your Tasks",id:"your-tasks",level:2},{value:"Task 1: Implement <code>build_model()</code>",id:"task-1-implement-build_model",level:3},{value:"Why &quot;Without Final Layer&quot;?",id:"why-without-final-layer",level:4},{value:"Implementation Approach:",id:"implementation-approach",level:4},{value:"Task 2: Implement <code>compute_embedding()</code>",id:"task-2-implement-compute_embedding",level:3},{value:"Implementation Approach:",id:"implementation-approach-1",level:4},{value:"Technical Details",id:"technical-details",level:2},{value:"Model Architecture:",id:"model-architecture",level:3},{value:"Tensor Shapes:",id:"tensor-shapes",level:3},{value:"Key Dependencies:",id:"key-dependencies",level:3},{value:"Testing",id:"testing",level:2},{value:"Expected Behavior",id:"expected-behavior",level:2},{value:"Next Steps",id:"next-steps",level:2},{value:"References",id:"references",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"exercise-02-convnext-model-and-embedding-generation",children:"Exercise 02. ConvNeXt Model and Embedding Generation"})}),"\n",(0,t.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsxs)(n.p,{children:["This exercise teaches you how to load a pre-trained ConvNeXt model and use it to generate face embeddings. You'll implement two key functions: ",(0,t.jsx)(n.code,{children:"build_model()"})," to load the model and ",(0,t.jsx)(n.code,{children:"compute_embedding()"})," to generate feature vectors from facial images."]}),"\n",(0,t.jsx)(n.h2,{id:"what-is-convnext",children:"What is ConvNeXt?"}),"\n",(0,t.jsx)(n.p,{children:"ConvNeXt (Convolution meets NeXt) is a modern convolutional neural network architecture that bridges the gap between traditional CNNs and Vision Transformers (ViTs). Introduced by Facebook AI Research in 2022, ConvNeXt modernizes the standard ResNet architecture by incorporating design choices inspired by Vision Transformers."}),"\n",(0,t.jsx)(n.h3,{id:"key-features-of-convnext",children:"Key Features of ConvNeXt:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Pure Convolutional Architecture"}),": Uses only convolutions, no self-attention mechanisms"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Modernized ResNet Design"}),": Incorporates macro and micro design choices from ViTs"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Competitive Performance"}),": Achieves performance comparable to Swin Transformers"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Efficiency"}),": Maintains the computational efficiency of traditional CNNs"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"convnext-atto-variant",children:"ConvNeXt-Atto Variant:"}),"\n",(0,t.jsxs)(n.p,{children:["We use ",(0,t.jsx)(n.strong,{children:"ConvNeXt-Atto"}),", an ultra-lightweight variant that provides excellent performance for face recognition tasks while being computationally efficient."]}),"\n",(0,t.jsx)(n.h2,{id:"what-are-face-embeddings",children:"What are Face Embeddings?"}),"\n",(0,t.jsx)(n.p,{children:"Embeddings are dense, low-dimensional vector representations that capture the essential characteristics of a face in numerical form."}),"\n",(0,t.jsx)(n.h3,{id:"purpose-of-face-embeddings",children:"Purpose of Face Embeddings:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Dimensionality Reduction"}),": Convert 224\xd7224\xd73 images (~150K pixels) to compact vectors (~320 dimensions)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Feature Extraction"}),": Capture essential facial characteristics (eye shape, nose structure, etc.)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Similarity Computation"}),": Enable mathematical comparison between different faces"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Efficient Storage"}),": Store compact representations instead of full images"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"properties-of-good-face-embeddings",children:"Properties of Good Face Embeddings:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Discriminative"}),": Different people produce different embeddings"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Robust"}),": Similar embeddings for the same person under different conditions"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Compact"}),": Much smaller than original images"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Comparable"}),": Can be compared using mathematical similarity metrics"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"your-tasks",children:"Your Tasks"}),"\n",(0,t.jsxs)(n.h3,{id:"task-1-implement-build_model",children:["Task 1: Implement ",(0,t.jsx)(n.code,{children:"build_model()"})]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-rust",children:"pub fn build_model() -> Result<Func<'static>>\n"})}),"\n",(0,t.jsx)(n.p,{children:"This function should:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Download Model"}),': Use Hugging Face Hub API to get "timm/convnext_atto.d2_in1k"']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Load Weights"}),": Load the SafeTensors model file"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Create Model"}),": Build ConvNeXt without the final classification layer"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Return Function"}),": Return a callable model function"]}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"why-without-final-layer",children:'Why "Without Final Layer"?'}),"\n",(0,t.jsx)(n.p,{children:"The original ConvNeXt model was trained for ImageNet classification (1000 classes). It has:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Feature Extraction Layers"}),": Extract meaningful patterns from images"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Final Classification Layer"}),": Maps features to 1000 ImageNet class probabilities"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"For face embeddings, we want:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"Feature Extraction"}),": The rich feature representations (embeddings)"]}),"\n",(0,t.jsxs)(n.li,{children:["\u274c ",(0,t.jsx)(n.strong,{children:"Classification"}),": We don't need ImageNet class predictions"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["By removing the final layer, we get the raw feature vectors (embeddings) that capture facial characteristics, which we can then use for similarity comparison: Use ",(0,t.jsx)(n.code,{children:"convnext::convnext_no_final_layer"})," - CHECK CANDLE CONVNEXT"]}),"\n",(0,t.jsx)(n.h4,{id:"implementation-approach",children:"Implementation Approach:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Use Hugging Face Hub API for model download"}),"\n",(0,t.jsx)(n.li,{children:"Load model weights with VarBuilder"}),"\n",(0,t.jsx)(n.li,{children:"Create ConvNeXt architecture without classification head"}),"\n",(0,t.jsx)(n.li,{children:"Return the model as a callable function"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Hint"}),": Check the CHEATSHEET.md for HuggingFace API patterns and model loading."]}),"\n",(0,t.jsxs)(n.h3,{id:"task-2-implement-compute_embedding",children:["Task 2: Implement ",(0,t.jsx)(n.code,{children:"compute_embedding()"})]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-rust",children:"pub fn compute_embedding(model: &Func, image: &Tensor) -> Result<Tensor>\n"})}),"\n",(0,t.jsx)(n.p,{children:"This function should:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Handle Input Format"}),": Check if input is single image or batch"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Add Batch Dimension"}),": If needed, ensure proper tensor dimensions"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Forward Pass"}),": Run the image through the model"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Return Embeddings"}),": Return the feature vectors"]}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"implementation-approach-1",children:"Implementation Approach:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Check tensor dimensions to determine if batching is needed"}),"\n",(0,t.jsx)(n.li,{children:"Ensure input tensor has the correct shape for the model"}),"\n",(0,t.jsx)(n.li,{children:"Use the model's forward method to generate embeddings"}),"\n",(0,t.jsx)(n.li,{children:"Return the resulting embedding tensor"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Hint"}),": Models typically expect batch dimensions. Check the CHEATSHEET.md for tensor dimension handling."]}),"\n",(0,t.jsx)(n.h2,{id:"technical-details",children:"Technical Details"}),"\n",(0,t.jsx)(n.h3,{id:"model-architecture",children:"Model Architecture:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Input"}),": 224\xd7224\xd73 RGB images (ImageNet normalized)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Output"}),": 768-dimensional embedding vectors"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Weights"}),": Pre-trained on ImageNet dataset"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Format"}),": SafeTensors for efficient loading"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"tensor-shapes",children:"Tensor Shapes:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Single Image Input"}),": ",(0,t.jsx)(n.code,{children:"[3, 224, 224]"})," \u2192 ",(0,t.jsx)(n.code,{children:"[1, 3, 224, 224]"})," (add batch dim)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Batch Input"}),": ",(0,t.jsx)(n.code,{children:"[N, 3, 224, 224]"})," \u2192 ",(0,t.jsx)(n.code,{children:"[N, 3, 224, 224]"})," (keep as is)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Output"}),": ",(0,t.jsx)(n.code,{children:"[N, 768]"})," where N is batch size"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"key-dependencies",children:"Key Dependencies:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"hf_hub"})," - Download models from Hugging Face"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"candle_transformers::models::convnext"})," - ConvNeXt implementation"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"candle_nn::VarBuilder"})," - Load model weights"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"testing",children:"Testing"}),"\n",(0,t.jsx)(n.p,{children:"The test verifies that:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Model loads successfully from Hugging Face"}),"\n",(0,t.jsx)(n.li,{children:"Embedding computation works with preprocessed images"}),"\n",(0,t.jsx)(n.li,{children:"Output tensor has the correct batch dimension"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Run the test with:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"cargo test\n"})}),"\n",(0,t.jsx)(n.h2,{id:"expected-behavior",children:"Expected Behavior"}),"\n",(0,t.jsx)(n.p,{children:"After successful implementation:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"build_model()"})," downloads and loads the ConvNeXt-Atto model"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"compute_embedding()"})," processes images and returns 768-dimensional embeddings"]}),"\n",(0,t.jsx)(n.li,{children:"The model handles both single images and batches automatically"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,t.jsx)(n.p,{children:"After completing this exercise, you'll be ready to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Learn similarity computation between embeddings (Exercise 03)"}),"\n",(0,t.jsx)(n.li,{children:"Understand how these embeddings enable face recognition"}),"\n",(0,t.jsx)(n.li,{children:"Build storage systems for embedding databases (Exercise 04)"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"ConvNeXt Paper"}),": ",(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2201.03545",children:"A ConvNet for the 2020s"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Hugging Face Model"}),": ",(0,t.jsx)(n.a,{href:"https://huggingface.co/timm/convnext_atto.d2_in1k",children:"timm/convnext_atto.d2_in1k"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Candle ConvNeXt"}),": ",(0,t.jsx)(n.a,{href:"https://github.com/huggingface/candle/blob/main/candle-transformers/src/models/convnext.rs",children:"GitHub Implementation"})]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>l,x:()=>d});var s=i(6540);const t={},r=s.createContext(t);function l(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function d(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:l(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);